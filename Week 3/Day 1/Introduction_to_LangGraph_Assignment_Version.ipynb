{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gJXW_DgiSebM"
      },
      "source": [
        "# LangGraph and LangSmith - Agentic RAG Powered by LangChain\n",
        "\n",
        "In the following notebook we'll complete the following tasks:\n",
        "\n",
        "- ü§ù Breakout Room #1:\n",
        "  1. Install required libraries\n",
        "  2. Set Environment Variables\n",
        "  3. Creating our Tool Belt\n",
        "  4. Creating Our State\n",
        "  5. Creating and Compiling A Graph!\n",
        "\n",
        "  - ü§ù Breakout Room #2:\n",
        "  1. Evaluating the LangGraph Application with LangSmith\n",
        "  2. Adding Helpfulness Check and \"Loop\" Limits\n",
        "  3. LangGraph for the \"Patterns\" of GenAI"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "djQ3nRAgoF67"
      },
      "source": [
        "# ü§ù Breakout Room #1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e7pQDUhUnIo8"
      },
      "source": [
        "## Part 1: LangGraph - Building Cyclic Applications with LangChain\n",
        "\n",
        "LangGraph is a tool that leverages LangChain Expression Language to build coordinated multi-actor and stateful applications that includes cyclic behaviour.\n",
        "\n",
        "### Why Cycles?\n",
        "\n",
        "In essence, we can think of a cycle in our graph as a more robust and customizable loop. It allows us to keep our application agent-forward while still giving the powerful functionality of traditional loops.\n",
        "\n",
        "Due to the inclusion of cycles over loops, we can also compose rather complex flows through our graph in a much more readable and natural fashion. Effectively allowing us to recreate application flowcharts in code in an almost 1-to-1 fashion.\n",
        "\n",
        "### Why LangGraph?\n",
        "\n",
        "Beyond the agent-forward approach - we can easily compose and combine traditional \"DAG\" (directed acyclic graph) chains with powerful cyclic behaviour due to the tight integration with LCEL. This means it's a natural extension to LangChain's core offerings!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3_fLDElOVoop"
      },
      "source": [
        "## Task 1:  Dependencies\n",
        "\n",
        "We'll first install all our required libraries."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "from IPython.display import display, Markdown\n",
        "\n",
        "def pretty_print(message: str) -> str:\n",
        "    display(Markdown(message))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "KaVwN269EttM"
      },
      "outputs": [],
      "source": [
        "!pip install -qU langchain langchain_openai langchain-community langgraph arxiv duckduckgo_search==5.3.1b1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wujPjGJuoPwg"
      },
      "source": [
        "## Task 2: Environment Variables\n",
        "\n",
        "We'll want to set both our OpenAI API key and our LangSmith environment variables."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jdh8CoVWHRvs",
        "outputId": "42b2ba5e-11ae-4f4d-e68e-77607bb794ad"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import getpass\n",
        "\n",
        "os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"OpenAI API Key:\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Nv0glIDyHmRt",
        "outputId": "30aa2260-50f1-4abf-b305-eed0ee1b9008"
      },
      "outputs": [],
      "source": [
        "from uuid import uuid4\n",
        "\n",
        "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
        "os.environ[\"LANGCHAIN_PROJECT\"] = f\"AIE4 - LangGraph - {uuid4().hex[0:8]}\"\n",
        "os.environ[\"LANGCHAIN_API_KEY\"] = getpass.getpass(\"LangSmith API Key: \")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sBRyQmEAVzua"
      },
      "source": [
        "## Task 3: Creating our Tool Belt\n",
        "\n",
        "As is usually the case, we'll want to equip our agent with a toolbelt to help answer questions and add external knowledge.\n",
        "\n",
        "There's a tonne of tools in the [LangChain Community Repo](https://github.com/langchain-ai/langchain/tree/master/libs/community/langchain_community/tools) but we'll stick to a couple just so we can observe the cyclic nature of LangGraph in action!\n",
        "\n",
        "We'll leverage:\n",
        "\n",
        "- [Duck Duck Go Web Search](https://github.com/langchain-ai/langchain/tree/master/libs/community/langchain_community/tools/ddg_search)\n",
        "- [Arxiv](https://github.com/langchain-ai/langchain/tree/master/libs/community/langchain_community/tools/arxiv)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2k6n_Dob2F46"
      },
      "source": [
        "####üèóÔ∏è Activity #1:\n",
        "\n",
        "Please add the tools to use into our toolbelt.\n",
        "\n",
        "> NOTE: Each tool in our toolbelt should be a method."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "lAxaSvlfIeOg"
      },
      "outputs": [],
      "source": [
        "from langchain_community.tools.ddg_search import DuckDuckGoSearchRun\n",
        "from langchain_community.tools.arxiv.tool import ArxivQueryRun\n",
        "\n",
        "tool_belt = [\n",
        "    DuckDuckGoSearchRun(),\n",
        "    ArxivQueryRun()\n",
        "]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VI-C669ZYVI5"
      },
      "source": [
        "### Model\n",
        "\n",
        "Now we can set-up our model! We'll leverage the familiar OpenAI model suite for this example - but it's not *necessary* to use with LangGraph. LangGraph supports all models - though you might not find success with smaller models - as such, they recommend you stick with:\n",
        "\n",
        "- OpenAI's GPT-3.5 and GPT-4\n",
        "- Anthropic's Claude\n",
        "- Google's Gemini\n",
        "\n",
        "> NOTE: Because we're leveraging the OpenAI function calling API - we'll need to use OpenAI *for this specific example* (or any other service that exposes an OpenAI-style function calling API."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "QkNS8rNZJs4z"
      },
      "outputs": [],
      "source": [
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "model = ChatOpenAI(model=\"gpt-4o\", temperature=0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ugkj3GzuZpQv"
      },
      "source": [
        "Now that we have our model set-up, let's \"put on the tool belt\", which is to say: We'll bind our LangChain formatted tools to the model in an OpenAI function calling format."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "4OdMqFafZ_0V"
      },
      "outputs": [],
      "source": [
        "model = model.bind_tools(tool_belt)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ERzuGo6W18Lr"
      },
      "source": [
        "#### ‚ùì Question #1:\n",
        "\n",
        "How does the model determine which tool to use?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Answer**\n",
        "\n",
        "Every tools comes with a description field, which is assigned at class declaration. If we use `@tool` decorator to convert a function into a tool then the docstring of the decorated function becomes the description of the tool. The LLM that the agent uses decides if and which tool to use on the basis of the prompt and the descriptions of the available tools."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_296Ub96Z_H8"
      },
      "source": [
        "## Task 4: Putting the State in Stateful\n",
        "\n",
        "Earlier we used this phrasing:\n",
        "\n",
        "`coordinated multi-actor and stateful applications`\n",
        "\n",
        "So what does that \"stateful\" mean?\n",
        "\n",
        "To put it simply - we want to have some kind of object which we can pass around our application that holds information about what the current situation (state) is. Since our system will be constructed of many parts moving in a coordinated fashion - we want to be able to ensure we have some commonly understood idea of that state.\n",
        "\n",
        "LangGraph leverages a `StatefulGraph` which uses an `AgentState` object to pass information between the various nodes of the graph.\n",
        "\n",
        "There are more options than what we'll see below - but this `AgentState` object is one that is stored in a `TypedDict` with the key `messages` and the value is a `Sequence` of `BaseMessages` that will be appended to whenever the state changes.\n",
        "\n",
        "Let's think about a simple example to help understand exactly what this means (we'll simplify a great deal to try and clearly communicate what state is doing):\n",
        "\n",
        "1. We initialize our state object:\n",
        "  - `{\"messages\" : []}`\n",
        "2. Our user submits a query to our application.\n",
        "  - New State: `HumanMessage(#1)`\n",
        "  - `{\"messages\" : [HumanMessage(#1)}`\n",
        "3. We pass our state object to an Agent node which is able to read the current state. It will use the last `HumanMessage` as input. It gets some kind of output which it will add to the state.\n",
        "  - New State: `AgentMessage(#1, additional_kwargs {\"function_call\" : \"WebSearchTool\"})`\n",
        "  - `{\"messages\" : [HumanMessage(#1), AgentMessage(#1, ...)]}`\n",
        "4. We pass our state object to a \"conditional node\" (more on this later) which reads the last state to determine if we need to use a tool - which it can determine properly because of our provided object!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 79,
      "metadata": {
        "id": "mxL9b_NZKUdL"
      },
      "outputs": [],
      "source": [
        "from typing import TypedDict, Annotated\n",
        "from langgraph.graph.message import add_messages\n",
        "import operator\n",
        "from langchain_core.messages import BaseMessage\n",
        "\n",
        "from collections import defaultdict\n",
        "\n",
        "class AgentState(TypedDict):\n",
        "  messages: Annotated[list, add_messages]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vWsMhfO9grLu"
      },
      "source": [
        "## Task 5: It's Graphing Time!\n",
        "\n",
        "Now that we have state, and we have tools, and we have an LLM - we can finally start making our graph!\n",
        "\n",
        "Let's take a second to refresh ourselves about what a graph is in this context.\n",
        "\n",
        "Graphs, also called networks in some circles, are a collection of connected objects.\n",
        "\n",
        "The objects in question are typically called nodes, or vertices, and the connections are called edges.\n",
        "\n",
        "Let's look at a simple graph.\n",
        "\n",
        "![image](https://i.imgur.com/2NFLnIc.png)\n",
        "\n",
        "Here, we're using the coloured circles to represent the nodes and the yellow lines to represent the edges. In this case, we're looking at a fully connected graph - where each node is connected by an edge to each other node.\n",
        "\n",
        "If we were to think about nodes in the context of LangGraph - we would think of a function, or an LCEL runnable.\n",
        "\n",
        "If we were to think about edges in the context of LangGraph - we might think of them as \"paths to take\" or \"where to pass our state object next\".\n",
        "\n",
        "Let's create some nodes and expand on our diagram.\n",
        "\n",
        "> NOTE: Due to the tight integration with LCEL - we can comfortably create our nodes in an async fashion!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 80,
      "metadata": {
        "id": "91flJWtZLUrl"
      },
      "outputs": [],
      "source": [
        "from langgraph.prebuilt import ToolNode\n",
        "\n",
        "def call_model(state):\n",
        "  messages = state[\"messages\"]\n",
        "  response = model.invoke(messages)\n",
        "  return {\"messages\" : [response]}\n",
        "\n",
        "tool_node = ToolNode(tool_belt)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2bwR7MgWj3Wg"
      },
      "source": [
        "Now we have two total nodes. We have:\n",
        "\n",
        "- `call_model` is a node that will...well...call the model\n",
        "- `tool_node` is a node which can call a tool\n",
        "\n",
        "Let's start adding nodes! We'll update our diagram along the way to keep track of what this looks like!\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 81,
      "metadata": {
        "id": "_vF4_lgtmQNo"
      },
      "outputs": [],
      "source": [
        "from langgraph.graph import StateGraph, END\n",
        "\n",
        "uncompiled_graph = StateGraph(AgentState)\n",
        "\n",
        "uncompiled_graph.add_node(\"agent\", call_model)\n",
        "uncompiled_graph.add_node(\"action\", tool_node)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b8CjRlbVmRpW"
      },
      "source": [
        "Let's look at what we have so far:\n",
        "\n",
        "![image](https://i.imgur.com/md7inqG.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uaXHpPeSnOWC"
      },
      "source": [
        "Next, we'll add our entrypoint. All our entrypoint does is indicate which node is called first."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 82,
      "metadata": {
        "id": "YGCbaYqRnmiw"
      },
      "outputs": [],
      "source": [
        "uncompiled_graph.set_entry_point(\"agent\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BUsfGoSpoF9U"
      },
      "source": [
        "![image](https://i.imgur.com/wNixpJe.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Q_pQgHmoW0M"
      },
      "source": [
        "Now we want to build a \"conditional edge\" which will use the output state of a node to determine which path to follow.\n",
        "\n",
        "We can help conceptualize this by thinking of our conditional edge as a conditional in a flowchart!\n",
        "\n",
        "Notice how our function simply checks if there is a \"function_call\" kwarg present.\n",
        "\n",
        "Then we create an edge where the origin node is our agent node and our destination node is *either* the action node or the END (finish the graph).\n",
        "\n",
        "It's important to highlight that the dictionary passed in as the third parameter (the mapping) should be created with the possible outputs of our conditional function in mind. In this case `should_continue` outputs either `\"end\"` or `\"continue\"` which are subsequently mapped to the action node or the END node."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 83,
      "metadata": {
        "id": "1BZgb81VQf9o"
      },
      "outputs": [],
      "source": [
        "def should_continue(state):\n",
        "  last_message = state[\"messages\"][-1]\n",
        "\n",
        "  print(f\"So far executed {len(state['messages'])} steps.\")\n",
        "\n",
        "  if last_message.tool_calls:\n",
        "    return \"action\"\n",
        "\n",
        "  return END\n",
        "\n",
        "uncompiled_graph.add_conditional_edges(\n",
        "    \"agent\",\n",
        "    should_continue\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-Cvhcf4jp0Ce"
      },
      "source": [
        "Let's visualize what this looks like.\n",
        "\n",
        "![image](https://i.imgur.com/8ZNwKI5.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yKCjWJCkrJb9"
      },
      "source": [
        "Finally, we can add our last edge which will connect our action node to our agent node. This is because we *always* want our action node (which is used to call our tools) to return its output to our agent!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 84,
      "metadata": {
        "id": "UvcgbHf1rIXZ"
      },
      "outputs": [],
      "source": [
        "uncompiled_graph.add_edge(\"action\", \"agent\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EiWDwBQtrw7Z"
      },
      "source": [
        "Let's look at the final visualization.\n",
        "\n",
        "![image](https://i.imgur.com/NWO7usO.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KYqDpErlsCsu"
      },
      "source": [
        "All that's left to do now is to compile our workflow - and we're off!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 85,
      "metadata": {
        "id": "zt9-KS8DpzNx"
      },
      "outputs": [],
      "source": [
        "compiled_graph = uncompiled_graph.compile()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xhNWIwBL1W4Q"
      },
      "source": [
        "#### ‚ùì Question #2:\n",
        "\n",
        "Is there any specific limit to how many times we can cycle?\n",
        "\n",
        "If not, how could we impose a limit to the number of cycles?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Answer**\n",
        "\n",
        "There is no specific limit to the number of cycles. However we can specify this limit and check it in the `should_continue` function:\n",
        "\n",
        "```python\n",
        "MAX_ITERATIONS = 10 # or any other positive number\n",
        "\n",
        "def should_continue(state):\n",
        "  last_message = state[\"messages\"][-1]\n",
        "\n",
        "  if len(state[\"messages\"]) > MAX_ITERATIONS:\n",
        "    return END\n",
        "\n",
        "  if last_message.tool_calls:\n",
        "    return \"action\"\n",
        "\n",
        "  return \n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VEYcTShCsPaa"
      },
      "source": [
        "## Using Our Graph\n",
        "\n",
        "Now that we've created and compiled our graph - we can call it *just as we'd call any other* `Runnable`!\n",
        "\n",
        "Let's try out a few examples to see how it fairs:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 88,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qn4n37PQRPII",
        "outputId": "a5d7ef7a-13f2-4066-df52-b0df89eae2ae"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "So far executed 2 steps.\n",
            "Receiving update from node: 'agent'\n"
          ]
        },
        {
          "data": {
            "text/markdown": [
              "[AIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'call_2gn5GJhTZOtZQ3HqLepEADtm', 'function': {'arguments': '{\"query\":\"current captain of the Winnipeg Jets 2023\"}', 'name': 'duckduckgo_search'}, 'type': 'function'}], 'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 25, 'prompt_tokens': 156, 'total_tokens': 181}, 'model_name': 'gpt-4o-2024-05-13', 'system_fingerprint': 'fp_157b3831f5', 'finish_reason': 'tool_calls', 'logprobs': None}, id='run-0bdbb4c2-de0e-425a-b68b-734ff6c1fd7b-0', tool_calls=[{'name': 'duckduckgo_search', 'args': {'query': 'current captain of the Winnipeg Jets 2023'}, 'id': 'call_2gn5GJhTZOtZQ3HqLepEADtm', 'type': 'tool_call'}], usage_metadata={'input_tokens': 156, 'output_tokens': 25, 'total_tokens': 181})]"
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Receiving update from node: 'action'\n"
          ]
        },
        {
          "data": {
            "text/markdown": [
              "[ToolMessage(content='Adam Lowry was named captain of the Winnipeg Jets on Tuesday. ... Sep 20, 2023. Latest News. Inside look at Vegas Golden Knights Aug 30, 2024. Vegas Golden Knights fantasy projections for 2024-25 Lowry will follow Andrew Ladd and Blake Wheeler to serve as the third captain of the new Winnipeg Jets franchise. - Sep 12, 2023. After a season without a captain, the Winnipeg Jets have named ... The Winnipeg Jets will have a captain for the 2023-24 season. After going captain-less in 2022-23, the Winnipeg Jets unveiled Adam Lowry as the club\\'s new captain on Tuesday morning. \"When I ... Posted September 12, 2023 9:29 am. Centre Adam Lowry was named the Winnipeg Jets new captain on Tuesday. Lowry is the third Jets captain since the team moved from Atlanta to Winnipeg in 2011. He follows Andrew Ladd and Blake Wheeler, who served as captain for five and six years respectively. ‚Äî Winnipeg Jets (@NHLJets) September 6, 2023. In some ways, the Jets are now one more step removed from the Paul Maurice and Wheeler-led era, and further into the next phase of the Jets. Which will evidently be led by Adam Lowry and Josh Morrissey, and for now, Mark Scheifele and Rick Bowness.', name='duckduckgo_search', tool_call_id='call_2gn5GJhTZOtZQ3HqLepEADtm')]"
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "So far executed 4 steps.\n",
            "Receiving update from node: 'agent'\n"
          ]
        },
        {
          "data": {
            "text/markdown": [
              "[AIMessage(content='The current captain of the Winnipeg Jets is Adam Lowry. He was named captain on September 12, 2023.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 26, 'prompt_tokens': 478, 'total_tokens': 504}, 'model_name': 'gpt-4o-2024-05-13', 'system_fingerprint': 'fp_157b3831f5', 'finish_reason': 'stop', 'logprobs': None}, id='run-b91ef67d-c68b-4ded-a595-62365d4346a8-0', usage_metadata={'input_tokens': 478, 'output_tokens': 26, 'total_tokens': 504})]"
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "from langchain_core.messages import HumanMessage\n",
        "\n",
        "inputs = {\"messages\" : [HumanMessage(content=\"Who is the current captain of the Winnipeg Jets?\")]}\n",
        "\n",
        "async for chunk in compiled_graph.astream(inputs, stream_mode=\"updates\"):\n",
        "    for node, values in chunk.items():\n",
        "        print(f\"Receiving update from node: '{node}'\")\n",
        "        #print(values[\"messages\"])\n",
        "        pretty_print(str(values[\"messages\"]))\n",
        "        # print(\"\\n\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DBHnUtLSscRr"
      },
      "source": [
        "Let's look at what happened:\n",
        "\n",
        "1. Our state object was populated with our request\n",
        "2. The state object was passed into our entry point (agent node) and the agent node added an `AIMessage` to the state object and passed it along the conditional edge\n",
        "3. The conditional edge received the state object, found the \"tool_calls\" `additional_kwarg`, and sent the state object to the action node\n",
        "4. The action node added the response from the OpenAI function calling endpoint to the state object and passed it along the edge to the agent node\n",
        "5. The agent node added a response to the state object and passed it along the conditional edge\n",
        "6. The conditional edge received the state object, could not find the \"tool_calls\" `additional_kwarg` and passed the state object to END where we see it output in the cell above!\n",
        "\n",
        "Now let's look at an example that shows a multiple tool usage - all with the same flow!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 106,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "afv2BuEsV5JG",
        "outputId": "026a3aa3-1c3e-4016-b0a3-fe98b1d25399"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "So far executed 2 steps.\n"
          ]
        },
        {
          "data": {
            "text/markdown": [
              "Receiving update from node: **agent**"
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "    Content:\n"
          ]
        },
        {
          "data": {
            "text/markdown": [],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "    Additional kwargs:\n"
          ]
        },
        {
          "data": {
            "text/markdown": [
              "tool_calls"
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/markdown": [
              "refusal"
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "    Tool calls:\n"
          ]
        },
        {
          "data": {
            "text/markdown": [
              "{'name': 'arxiv', 'args': {'query': 'QLoRA'}, 'id': 'call_44nXHWXEiNrXi5CyFpMcgIHb', 'type': 'tool_call'}"
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/markdown": [
              "{'name': 'duckduckgo_search', 'args': {'query': 'latest Tweet'}, 'id': 'call_4YrjCmmCknpTDHgyhxNAi92s', 'type': 'tool_call'}"
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\n"
          ]
        },
        {
          "data": {
            "text/markdown": [
              "Receiving update from node: **action**"
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "    Tools Used:\n"
          ]
        },
        {
          "data": {
            "text/markdown": [
              "arxiv"
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/markdown": [
              "duckduckgo_search"
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "    Content:\n"
          ]
        },
        {
          "data": {
            "text/markdown": [
              "Published: 2023-05-23\n",
              "Title: QLoRA: Efficient Finetuning of Quantized LLMs\n",
              "Authors: Tim Dettmers, Artidoro Pagnoni, Ari Holtzman, Luke Zettlemoyer\n",
              "Summary: We present QLoRA, an efficient finetuning approach that reduces memory usage\n",
              "enough to finetune a 65B parameter model on a single 48GB GPU while preserving\n",
              "full 16-bit finetuning task performance. QLoRA backpropagates gradients through\n",
              "a frozen, 4-bit quantized pretrained language model into Low Rank\n",
              "Adapters~(LoRA). Our best model family, which we name Guanaco, outperforms all\n",
              "previous openly released models on the Vicuna benchmark, reaching 99.3% of the\n",
              "performance level of ChatGPT while only requiring 24 hours of finetuning on a\n",
              "single GPU. QLoRA introduces a number of innovations to save memory without\n",
              "sacrificing performance: (a) 4-bit NormalFloat (NF4), a new data type that is\n",
              "information theoretically optimal for normally distributed weights (b) double\n",
              "quantization to reduce the average memory footprint by quantizing the\n",
              "quantization constants, and (c) paged optimziers to manage memory spikes. We\n",
              "use QLoRA to finetune more than 1,000 models, providing a detailed analysis of\n",
              "instruction following and chatbot performance across 8 instruction datasets,\n",
              "multiple model types (LLaMA, T5), and model scales that would be infeasible to\n",
              "run with regular finetuning (e.g. 33B and 65B parameter models). Our results\n",
              "show that QLoRA finetuning on a small high-quality dataset leads to\n",
              "state-of-the-art results, even when using smaller models than the previous\n",
              "SoTA. We provide a detailed analysis of chatbot performance based on both human\n",
              "and GPT-4 evaluations showing that GPT-4 evaluations are a cheap and reasonable\n",
              "alternative to human evaluation. Furthermore, we find that current chatbot\n",
              "benchmarks are not trustworthy to accurately evaluate the performance levels of\n",
              "chatbots. A lemon-picked analysis demonstrates where Guanaco fails compared to\n",
              "ChatGPT. We release all of our models and code, including CUDA kernels for\n",
              "4-bit training.\n",
              "\n",
              "Published: 2024-05-27\n",
              "Title: Accurate LoRA-Finetuning Quantization of LLMs via Information Retention\n",
              "Authors: Haotong Qin, Xudong Ma, Xingyu Zheng, Xiaoyang Li, Yang Zhang, Shouda Liu, Jie Luo, Xianglong Liu, Michele Magno\n",
              "Summary: The LoRA-finetuning quantization of LLMs has been extensively studied to\n",
              "obtain accurate yet compact LLMs for deployment on resource-constrained\n",
              "hardware. However, existing methods cause the quantized LLM to severely degrade\n",
              "and even fail to benefit from the finetuning of LoRA. This paper proposes a\n",
              "novel IR-QLoRA for pushing quantized LLMs with LoRA to be highly accurate\n",
              "through information retention. The proposed IR-QLoRA mainly relies on two\n",
              "technologies derived from the perspective of unified information: (1)\n",
              "statistics-based Information Calibration Quantization allows the quantized\n",
              "parameters of LLM to retain original information accurately; (2)\n",
              "finetuning-based Information Elastic Connection makes LoRA utilizes elastic\n",
              "representation transformation with diverse information. Comprehensive\n",
              "experiments show that IR-QLoRA can significantly improve accuracy across LLaMA\n",
              "and LLaMA2 families under 2-4 bit-widths, e.g., 4- bit LLaMA-7B achieves 1.4%\n",
              "improvement on MMLU compared with the state-of-the-art methods. The significant\n",
              "performance gain requires only a tiny 0.31% additional time consumption,\n",
              "revealing the satisfactory efficiency of our IR-QLoRA. We highlight that\n",
              "IR-QLoRA enjoys excellent versatility, compatible with various frameworks\n",
              "(e.g., NormalFloat and Integer quantization) and brings general accuracy gains.\n",
              "The code is available at https://github.com/htqin/ir-qlora.\n",
              "\n",
              "Published: 2024-06-12\n",
              "Title: Exploring Fact Memorization and Style Imitation in LLMs Using QLoRA: An Experimental Study and Quality Assessment Methods\n",
              "Authors: Eugene Vyborov, Oleksiy Osypenko, Serge Sotnyk\n",
              "Summary: There are various methods for adapting LLMs to different domains. The most\n",
              "common methods are prompting, finetuning, and RAG. In this w"
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "    Additional kwargs:\n",
            "    Tool calls:\n",
            "    No tool calls requested\n",
            "    Content:\n"
          ]
        },
        {
          "data": {
            "text/markdown": [
              "The latest Twitter news and updates. Twitter is a social networking service, primarily microblogging but also a picture and video sharing service, founded by Jack Dorsey, Noah Glass, Biz Stone and ... Twitter adds 'glorifying violence' warning to Trump tweet. Twitter has added a warning to one of President Donald Trump's tweets about protests in Minneapolis. ... In his latest broadside ... In more serious matters, EU Commissioner Thierry Breton visited Twitter's headquarters to conduct a stress test for the new Digital Services Act regulating everything from social media content ... Introducing a new form of Free (v2) access for write-only use cases and those testing the Twitter API with 1,500 Tweets/month at the app level, media upload endpoints, and Login with Twitter. Get ... Sept. 15, 2023. The federal prosecutors who charged former President Donald J. Trump with a criminal conspiracy over his attempts to overturn the 2020 election obtained 32 private messages from ..."
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "    Additional kwargs:\n",
            "    Tool calls:\n",
            "    No tool calls requested\n",
            "\n",
            "\n",
            "\n",
            "So far executed 5 steps.\n"
          ]
        },
        {
          "data": {
            "text/markdown": [
              "Receiving update from node: **agent**"
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "    Content:\n"
          ]
        },
        {
          "data": {
            "text/markdown": [],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "    Additional kwargs:\n"
          ]
        },
        {
          "data": {
            "text/markdown": [
              "tool_calls"
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/markdown": [
              "refusal"
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "    Tool calls:\n"
          ]
        },
        {
          "data": {
            "text/markdown": [
              "{'name': 'duckduckgo_search', 'args': {'query': 'Tim Dettmers latest Tweet'}, 'id': 'call_FUmCGk79X5eaMb8uB38SpqCb', 'type': 'tool_call'}"
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/markdown": [
              "{'name': 'duckduckgo_search', 'args': {'query': 'Artidoro Pagnoni latest Tweet'}, 'id': 'call_i12ExzlqnJR5JRZTJLryhn0n', 'type': 'tool_call'}"
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/markdown": [
              "{'name': 'duckduckgo_search', 'args': {'query': 'Ari Holtzman latest Tweet'}, 'id': 'call_nJ9f6oxmACdQtRHasaaeGYGK', 'type': 'tool_call'}"
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/markdown": [
              "{'name': 'duckduckgo_search', 'args': {'query': 'Luke Zettlemoyer latest Tweet'}, 'id': 'call_DzFCjSkMVofz4CUuRlwXlrQp', 'type': 'tool_call'}"
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\n"
          ]
        },
        {
          "data": {
            "text/markdown": [
              "Receiving update from node: **action**"
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "    Tools Used:\n"
          ]
        },
        {
          "data": {
            "text/markdown": [
              "duckduckgo_search"
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/markdown": [
              "duckduckgo_search"
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/markdown": [
              "duckduckgo_search"
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/markdown": [
              "duckduckgo_search"
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "    Content:\n"
          ]
        },
        {
          "data": {
            "text/markdown": [
              "Tim Walz's conservative older brother, Jeff, told NewsNation that the \"stories\" he alluded to in a recent Facebook post are limited to stuff like puking on his siblings due to car sickness when ... Tim Walz was born in West Point, Neb. and grew up in rural Valentine. He previously described his upbringing as coming \"from a town of 400 ‚Äî 24 kids in a class, 12 cousins, farming, those ... Allen School Ph.D. student Tim Dettmers accepted the grand prize for QLoRA, a novel approach to finetuning pretrained models that significantly reduces the amount of GPU memory required ‚Äî from over 780GB to less than 48GB ‚Äî to finetune a 65B parameter model. With QLoRA, the largest publicly available models can be finetuned on a single ... In the chat quoted in the complaint, researcher Tim Dettmers talks about his back-and-forth with Meta's legal department whether the use of the book files as training data would be \"legally ok ... Tech Moves: AI researcher Yejin Choi leaves Univ. of Washington and Allen Institute for AI. by Todd Bishop & Taylor Soper on August 2, 2024. Yejin Choi, who was named a 2022 MacArthur Fellow and ..."
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "    Additional kwargs:\n",
            "    Tool calls:\n",
            "    No tool calls requested\n",
            "    Content:\n"
          ]
        },
        {
          "data": {
            "text/markdown": [
              "Artidoro Pagnoni. artidoro. Follow. mrm8488's profile picture Weyaxi's profile picture nezubn's profile picture. ... artidoro/model-tvergho. Updated Nov 18, 2023. artidoro/model-vinaic. Updated Nov 18, 2023. artidoro/model-vinaia. Updated Nov 18, 2023. datasets. None public yet. Company We present QLoRA, an efficient finetuning approach that reduces memory usage enough to finetune a 65B parameter model on a single 48GB GPU while preserving full 16-bit finetuning task performance. QLoRA backpropagates gradients through a frozen, 4-bit quantized pretrained language model into Low Rank Adapters~(LoRA). Our best model family, which we name Guanaco, outperforms all previous openly ... In this paper, we address these aforementioned challenges associated with financial data and introduce FinGPT, an end-to-end open-source framework for financial large language models (FinLLMs). Adopting a data-centric approach, FinGPT underscores the crucial role of data acquisition, cleaning, and preprocessing in developing open-source FinLLMs. efficient finetuning of quantized LLMs. AUTHORs: Tim Dettmers, Artidoro Pagnoni, Ari Holtzman, Luke Zettlemoyer Authors Info & Claims. NIPS '23: Proceedings of the 37th International Conference on Neural Information Processing Systems. Article No.: 441, Pages 10088 - 10115. Published: 30 May 2024 Publication History. Saturday, June 29, 2024. Introduction. Fine-tuning large language models (LLMs) is a common practice to adapt them for specific tasks, but it can be computationally expensive. LoRA (Low-Rank Adaptation) is a technique that makes this process more efficient by introducing small adapter modules to the model. These adapters capture task-specific ..."
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "    Additional kwargs:\n",
            "    Tool calls:\n",
            "    No tool calls requested\n",
            "    Content:\n"
          ]
        },
        {
          "data": {
            "text/markdown": [
              "MSNBC host Ari Melber, during an interview with Trump campaign adviser Corey Lewandowski on Wednesday, threatened him with a defamation lawsuit for quoting the anchor calling the former President ... The team behind QLoRA includes Allen School Ph.D. student Artidoro Pagnoni; alum Ari Holtzman (Ph.D., '23), incoming professor at the University of Chicago; and professor Luke Zettlemoyer, who is also a research manager at Meta. Madrona Prize First Runner Up / Punica: Multi-Tenant LoRA Fine-tuned LLM Serving Corey Lewandowski, who joined Trump's presidential campaign team just two weeks ago after he was fired from his campaign in 2016, appeared on Wednesday's episode of The Beat The heated on-air dispute last night between MSNBC's Ari Melber and Donald Trump campaign adviser Corey Lewandowski didn't end with Wednesday's segment: Today, Lewandowski tweeted a video in ... The Astros are set to recall catcher Cesar Salazar from Triple-A Sugar Land for the September 1 roster expansion, allowing more flexibility for main catchers Yainer Diaz and Victor Caratini."
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "    Additional kwargs:\n",
            "    Tool calls:\n",
            "    No tool calls requested\n",
            "    Content:\n"
          ]
        },
        {
          "data": {
            "text/markdown": [
              "Luke Zettlemoyer is a research manager and site lead for FAIR Seattle. He is also a Professor in the Allen School of Computer Science & Engineering at the University of Washington. His research is in empirical computational semantics, where the goal is to build models that recover representations of the meaning of natural language text. Today we're joined by Luke Zettlemoyer, professor at University of Washington and a research manager at Meta. In our conversation with Luke, we cover multimodal generative AI, the effect of data on models, and the significance of open source and open science. We explore the grounding problem, the need for visual grounding and embodiment in View a PDF of the paper titled MoMa: Efficient Early-Fusion Pre-training with Mixture of Modality-Aware Experts, by Xi Victoria Lin and Akshat Shrivastava and Liang Luo and Srinivasan Iyer and Mike Lewis and Gargi Ghosh and Luke Zettlemoyer and Armen Aghajanyan Ôªø Twitter Ôªø Reddit. Join our list for notifications and early access to events ... About this Episode. Today we're joined by Luke Zettlemoyer, professor at University of Washington and a research manager at Meta. In our conversation with Luke, we cover multimodal generative AI, the effect of data on models, and the significance of open ... Provost Tricia Serio and President Ana Mari Cauce have appointed a Task Force on Artificial Intelligence to address these issues and to suggest a UW-wide AI strategy. Chaired by Andreas Bohman, vice president of UW-IT and the University's chief information officer, and Anind Dey, dean of the Information School, the task force will initially ..."
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "    Additional kwargs:\n",
            "    Tool calls:\n",
            "    No tool calls requested\n",
            "\n",
            "\n",
            "\n",
            "So far executed 10 steps.\n"
          ]
        },
        {
          "data": {
            "text/markdown": [
              "Receiving update from node: **agent**"
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "    Content:\n"
          ]
        },
        {
          "data": {
            "text/markdown": [
              "Here are the latest updates related to the authors of the QLoRA paper:\n",
              "\n",
              "### Tim Dettmers\n",
              "- **Latest News**: Tim Dettmers, an Allen School Ph.D. student, accepted the grand prize for QLoRA, a novel approach to finetuning pretrained models that significantly reduces the amount of GPU memory required‚Äîfrom over 780GB to less than 48GB‚Äîto finetune a 65B parameter model. With QLoRA, the largest publicly available models can be finetuned on a single GPU.\n",
              "\n",
              "### Artidoro Pagnoni\n",
              "- **Latest News**: Artidoro Pagnoni is involved in the development of QLoRA, an efficient finetuning approach that reduces memory usage enough to finetune a 65B parameter model on a single 48GB GPU while preserving full 16-bit finetuning task performance. He is also associated with other projects like FinGPT, an end-to-end open-source framework for financial large language models (FinLLMs).\n",
              "\n",
              "### Ari Holtzman\n",
              "- **Latest News**: Ari Holtzman, an alum of the Allen School (Ph.D., '23) and incoming professor at the University of Chicago, is part of the team behind QLoRA. He has also been mentioned in the context of a heated on-air dispute involving MSNBC host Ari Melber and Trump campaign adviser Corey Lewandowski.\n",
              "\n",
              "### Luke Zettlemoyer\n",
              "- **Latest News**: Luke Zettlemoyer is a research manager and site lead for FAIR Seattle, as well as a Professor in the Allen School of Computer Science & Engineering at the University of Washington. His research focuses on empirical computational semantics. He has been involved in discussions about multimodal generative AI, the effect of data on models, and the significance of open source and open science.\n",
              "\n",
              "Unfortunately, I couldn't find their latest Tweets directly. If you need more specific information or direct links to their social media profiles, you might want to check their official Twitter accounts or other social media platforms."
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "    Additional kwargs:\n"
          ]
        },
        {
          "data": {
            "text/markdown": [
              "refusal"
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "    Tool calls:\n",
            "\n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "inputs = {\"messages\" : [HumanMessage(content=\"Search Arxiv for the QLoRA paper, then search each of the authors to find out their latest Tweet using DuckDuckGo.\")]}\n",
        "\n",
        "async for chunk in compiled_graph.astream(inputs, stream_mode=\"updates\"):\n",
        "    for node, values in chunk.items():\n",
        "        pretty_print(f\"Receiving update from node: **{node}**\")\n",
        "        if node == \"action\":\n",
        "          print(\"    Tools Used:\")\n",
        "          for x in values['messages']: pretty_print(f\"{x.name}\")\n",
        "        # print(\">>>\")\n",
        "        # pretty_print(str(values[\"messages\"]))\n",
        "        # print(\"<<<\")\n",
        "        for x in values[\"messages\"]:\n",
        "          print(\"    Content:\")\n",
        "          if isinstance(x.content, str):\n",
        "             pretty_print(x.content)\n",
        "          else:\n",
        "            for y in x.content: pretty_print(f\"{y}\")\n",
        "          print(\"    Additional kwargs:\")\n",
        "          for y in x.additional_kwargs: pretty_print(f\"{y}\")\n",
        "          try:\n",
        "            print(\"    Tool calls:\")\n",
        "            for y in x.tool_calls: pretty_print(f\"{y}\")\n",
        "          except:\n",
        "            print(\"    No tool calls requested\")\n",
        "        print(\"\\n\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CXzDlZVz1Hnf"
      },
      "source": [
        "####üèóÔ∏è Activity #2:\n",
        "\n",
        "Please write out the steps the agent took to arrive at the correct answer:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Answer**\n",
        "\n",
        "I multiple executions of this workflow the agent arrived at wrong answers, in different ways:\n",
        "- Agent asks for the latest tweets of people she considers authors of QLoRA paper, before verifying the list of authors. As a result the it looks for tweets from wrong people.\n",
        "- Agent asks for \"latest tweets\" without specifying names of people.\n",
        "- Agent asks for \"latest tweets\" of correct people but collects wrong data.\n",
        "- Agent faces duckduck go rate limit and fails to collect the tweets, does not tries again.\n",
        "\n",
        "The execution of the graph looks different at different runs. Here is the list of steps performed in the run shown above:\n",
        "1. User query received and read.\n",
        "2. Agent decides to use two tools:\n",
        "    - pull the QLoRA paper (abstract) from arxiv\n",
        "    - search for \"latest tweets\" using duckduckgo\n",
        "3. Based on the QLoRA abstract (contains list of authors) and a useless search result for \"latest tweets\" the agent decides to search the Internet for the latest tweet of each of the authors.\n",
        "4. Based on collected output - which seems not to contain the actual tweets - it builds a response which talks about \"latest updates\" instead of \"latest tweets\".\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v7c8-Uyarh1v"
      },
      "source": [
        "## Part 1: LangSmith Evaluator"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pV3XeFOT1Sar"
      },
      "source": [
        "### Pre-processing for LangSmith"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wruQCuzewUuO"
      },
      "source": [
        "To do a little bit more preprocessing, let's wrap our LangGraph agent in a simple chain."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 107,
      "metadata": {
        "id": "oeXdQgbxwhTv"
      },
      "outputs": [],
      "source": [
        "def convert_inputs(input_object):\n",
        "  return {\"messages\" : [HumanMessage(content=input_object[\"question\"])]}\n",
        "\n",
        "def parse_output(input_state):\n",
        "  return input_state[\"messages\"][-1].content\n",
        "\n",
        "agent_chain = convert_inputs | compiled_graph | parse_output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 108,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        },
        "id": "orYxBZXSxJjZ",
        "outputId": "b160f3e4-89d1-4411-ed96-fe17b3cad200"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "So far executed 2 steps.\n"
          ]
        },
        {
          "data": {
            "text/markdown": [
              "RAG stands for Retrieval-Augmented Generation. It is a technique used in natural language processing (NLP) and machine learning to improve the performance of language models by combining retrieval-based methods with generative models. Here's a brief overview of how it works:\n",
              "\n",
              "1. **Retrieval**: In the first step, the system retrieves relevant documents or pieces of information from a large corpus based on the input query. This is typically done using a retrieval model, such as BM25 or a dense retrieval model like DPR (Dense Passage Retrieval).\n",
              "\n",
              "2. **Augmentation**: The retrieved documents are then used to augment the input query. This means that the information from the retrieved documents is combined with the original query to provide more context and relevant information.\n",
              "\n",
              "3. **Generation**: Finally, a generative model, such as GPT-3 or BERT, uses the augmented input to generate a response. The generative model can produce more accurate and contextually relevant answers because it has access to additional information retrieved in the first step.\n",
              "\n",
              "RAG is particularly useful in scenarios where the input query is complex or requires specific knowledge that may not be fully captured by the generative model alone. By leveraging external information sources, RAG can enhance the quality and relevance of the generated responses."
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "pretty_print(agent_chain.invoke({\"question\" : \"What is RAG?\"}))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f9UkCIqkpyZu"
      },
      "source": [
        "### Task 1: Creating An Evaluation Dataset\n",
        "\n",
        "Just as we saw last week, we'll want to create a dataset to test our Agent's ability to answer questions.\n",
        "\n",
        "In order to do this - we'll want to provide some questions and some answers. Let's look at how we can create such a dataset below.\n",
        "\n",
        "```python\n",
        "questions = [\n",
        "    \"What optimizer is used in QLoRA?\",\n",
        "    \"What data type was created in the QLoRA paper?\",\n",
        "    \"What is a Retrieval Augmented Generation system?\",\n",
        "    \"Who authored the QLoRA paper?\",\n",
        "    \"What is the most popular deep learning framework?\",\n",
        "    \"What significant improvements does the LoRA system make?\"\n",
        "]\n",
        "\n",
        "answers = [\n",
        "    {\"must_mention\" : [\"paged\", \"optimizer\"]},\n",
        "    {\"must_mention\" : [\"NF4\", \"NormalFloat\"]},\n",
        "    {\"must_mention\" : [\"ground\", \"context\"]},\n",
        "    {\"must_mention\" : [\"Tim\", \"Dettmers\"]},\n",
        "    {\"must_mention\" : [\"PyTorch\", \"TensorFlow\"]},\n",
        "    {\"must_mention\" : [\"reduce\", \"parameters\"]},\n",
        "]\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VfMXF2KAsQxs"
      },
      "source": [
        "####üèóÔ∏è Activity #3:\n",
        "\n",
        "Please create a dataset in the above format with at least 5 questions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 115,
      "metadata": {
        "id": "CbagRuJop83E"
      },
      "outputs": [],
      "source": [
        "questions = [\n",
        "    \"What is the capital city of Poland?\",\n",
        "    \"How to store key-value pairs in Python?\",\n",
        "    \"What capabilities agents add to LLM-based applications?\",\n",
        "    \"Who is the current president of United States\",\n",
        "    \"What is the most popular framework for distributed computations?\",\n",
        "]\n",
        "\n",
        "answers = [\n",
        "    {\"must_mention\" : [\"Warsaw\"]},\n",
        "    {\"must_mention\" : [\"dictionary\", \"dict\"]},\n",
        "    {\"must_mention\" : [\"tool\", \"reasoning\"]},\n",
        "    {\"must_mention\" : [\"Joe\", \"Biden\"]},\n",
        "    {\"must_mention\" : [\"Apache\", \"Spark\"]},\n",
        "]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z7QVFuAmsh7L"
      },
      "source": [
        "Now we can add our dataset to our LangSmith project using the following code which we saw last Thursday!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 116,
      "metadata": {
        "id": "RLfrZrgSsn85"
      },
      "outputs": [],
      "source": [
        "from langsmith import Client\n",
        "\n",
        "client = Client()\n",
        "dataset_name = f\"Retrieval Augmented Generation - Evaluation Dataset - {uuid4().hex[0:8]}\"\n",
        "\n",
        "dataset = client.create_dataset(\n",
        "    dataset_name=dataset_name,\n",
        "    description=\"Questions about the QLoRA Paper to Evaluate RAG over the same paper.\"\n",
        ")\n",
        "\n",
        "client.create_examples(\n",
        "    inputs=[{\"question\" : q} for q in questions],\n",
        "    outputs=answers,\n",
        "    dataset_id=dataset.id,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ciV73F9Q04w0"
      },
      "source": [
        "#### ‚ùì Question #3:\n",
        "\n",
        "How are the correct answers associated with the questions?\n",
        "\n",
        "> NOTE: Feel free to indicate if this is problematic or not"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Answer**\n",
        "\n",
        "Not sure I understand the question. For every question we define criteria of the answer being correct. Here we ask for specific terms to appear in the answer. Such criterium may have following shortcomings:\n",
        "- answer may contain lots of irrelevant information and still pass as correct\n",
        "- answer may by correct while not containing all required terms"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 117,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "So far executed 2 steps.\n"
          ]
        },
        {
          "data": {
            "text/markdown": [
              "In LangSmith evaluation, the correct answers are typically associated with questions through a process that involves defining a set of expected answers or ground truth for each question. This process can vary depending on the specific evaluation framework or methodology being used, but generally includes the following steps:\n",
              "\n",
              "1. **Question Formulation**: Clearly define the questions that need to be evaluated. These questions should be specific and unambiguous.\n",
              "\n",
              "2. **Ground Truth Creation**: Establish the correct answers for each question. This can be done by subject matter experts who provide the most accurate and reliable answers based on their knowledge and available information.\n",
              "\n",
              "3. **Answer Mapping**: Each question is mapped to its corresponding correct answer. This mapping is crucial for the evaluation process as it serves as the reference against which the responses will be compared.\n",
              "\n",
              "4. **Evaluation Criteria**: Define the criteria for evaluating the responses. This can include exact match, partial match, relevance, correctness, completeness, and other relevant metrics.\n",
              "\n",
              "5. **Automated or Manual Evaluation**: Use automated tools or manual review to compare the provided answers with the correct answers. Automated tools can include algorithms that check for exact matches or use natural language processing techniques to assess the similarity between the provided and correct answers.\n",
              "\n",
              "6. **Scoring and Feedback**: Assign scores based on the evaluation criteria and provide feedback on the performance. This helps in understanding how well the responses align with the correct answers.\n",
              "\n",
              "In some advanced evaluation systems, machine learning models may be used to predict the correctness of answers based on training data that includes a large set of questions and their corresponding correct answers.\n",
              "\n",
              "If you have a specific context or framework in mind for LangSmith evaluation, please provide more details so I can give a more tailored explanation."
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "pretty_print(agent_chain.invoke({\"question\" : \"How are the correct answers associated with the questions in LangSmith evaluation?\"}))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-lRTXUrTtP9Y"
      },
      "source": [
        "### Task 2: Adding Evaluators\n",
        "\n",
        "Now we can add a custom evaluator to see if our responses contain the expected information.\n",
        "\n",
        "We'll be using a fairly naive exact-match process to determine if our response contains specific strings."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 118,
      "metadata": {
        "id": "QrAUXMFftlAY"
      },
      "outputs": [],
      "source": [
        "from langsmith.evaluation import EvaluationResult, run_evaluator\n",
        "\n",
        "@run_evaluator\n",
        "def must_mention(run, example) -> EvaluationResult:\n",
        "    prediction = run.outputs.get(\"output\") or \"\"\n",
        "    required = example.outputs.get(\"must_mention\") or []\n",
        "    score = all(phrase in prediction for phrase in required)\n",
        "    return EvaluationResult(key=\"must_mention\", score=score)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PNtHORUh0jZY"
      },
      "source": [
        "#### ‚ùì Question #4:\n",
        "\n",
        "What are some ways you could improve this metric as-is?\n",
        "\n",
        "> NOTE: Alternatively you can suggest where gaps exist in this method."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Answer**\n",
        "\n",
        "Ways to improve the 'must_mention' metric:\n",
        "- allow for ignoring letter casing\n",
        "- allow for other options than mentioning all listed terms e.g. at least one term must appear in the answer for it pass as correct"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mZ4DVSXl0BX5"
      },
      "source": [
        "Now that we have created our custom evaluator - let's initialize our `RunEvalConfig` with it!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 119,
      "metadata": {
        "id": "sL4-XcjytWsu"
      },
      "outputs": [],
      "source": [
        "from langchain.smith import RunEvalConfig, run_on_dataset\n",
        "\n",
        "eval_config = RunEvalConfig(\n",
        "    custom_evaluators=[must_mention],\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r1RJr349zhv7"
      },
      "source": [
        "Task 3: Evaluating\n",
        "\n",
        "All that is left to do is evaluate our agent's response!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 120,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p5TeCUUkuGld",
        "outputId": "8b98fff1-bd75-4dbe-cadc-a76d8a82577c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "View the evaluation results for project 'RAG Pipeline - Evaluation - 1a6d3ccf' at:\n",
            "https://smith.langchain.com/o/36ad7c64-e702-5d92-ad6e-24634a4b396a/datasets/c3a44be7-db1f-4956-802f-b270727d739f/compare?selectedSessions=5404d41a-497b-4fe2-b31e-bacdff10bfee\n",
            "\n",
            "View all tests for Dataset Retrieval Augmented Generation - Evaluation Dataset - 661145e9 at:\n",
            "https://smith.langchain.com/o/36ad7c64-e702-5d92-ad6e-24634a4b396a/datasets/c3a44be7-db1f-4956-802f-b270727d739f\n",
            "[>                                                 ] 0/5So far executed 2 steps.\n",
            "So far executed 2 steps.\n",
            "So far executed 2 steps.\n",
            "[--------->                                        ] 1/5So far executed 4 steps.\n",
            "So far executed 4 steps.\n",
            "[------------------->                              ] 2/5So far executed 2 steps.\n",
            "[--------------------------------------->          ] 4/5So far executed 2 steps.\n",
            "[------------------------------------------------->] 5/5"
          ]
        },
        {
          "data": {
            "text/html": [
              "<h3>Experiment Results:</h3>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>feedback.must_mention</th>\n",
              "      <th>error</th>\n",
              "      <th>execution_time</th>\n",
              "      <th>run_id</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>5</td>\n",
              "      <td>0</td>\n",
              "      <td>5.000000</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>unique</th>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>top</th>\n",
              "      <td>True</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>9d6f84ca-a5be-441a-86ac-325b8801e48e</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>freq</th>\n",
              "      <td>4</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean</th>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>4.099279</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>std</th>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>3.334678</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>min</th>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.631296</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25%</th>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>2.761924</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50%</th>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>2.820014</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75%</th>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>4.848421</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>max</th>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>9.434741</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "       feedback.must_mention error  execution_time  \\\n",
              "count                      5     0        5.000000   \n",
              "unique                     2     0             NaN   \n",
              "top                     True   NaN             NaN   \n",
              "freq                       4   NaN             NaN   \n",
              "mean                     NaN   NaN        4.099279   \n",
              "std                      NaN   NaN        3.334678   \n",
              "min                      NaN   NaN        0.631296   \n",
              "25%                      NaN   NaN        2.761924   \n",
              "50%                      NaN   NaN        2.820014   \n",
              "75%                      NaN   NaN        4.848421   \n",
              "max                      NaN   NaN        9.434741   \n",
              "\n",
              "                                      run_id  \n",
              "count                                      5  \n",
              "unique                                     5  \n",
              "top     9d6f84ca-a5be-441a-86ac-325b8801e48e  \n",
              "freq                                       1  \n",
              "mean                                     NaN  \n",
              "std                                      NaN  \n",
              "min                                      NaN  \n",
              "25%                                      NaN  \n",
              "50%                                      NaN  \n",
              "75%                                      NaN  \n",
              "max                                      NaN  "
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "{'project_name': 'RAG Pipeline - Evaluation - 1a6d3ccf',\n",
              " 'results': {'9ab6ae6f-a014-4f65-b111-f6e223c9afa0': {'input': {'question': 'What is the capital city of Poland?'},\n",
              "   'feedback': [EvaluationResult(key='must_mention', score=True, value=None, comment=None, correction=None, evaluator_info={}, feedback_config=None, source_run_id=UUID('3943bddd-0615-427e-b90d-7384266f4920'), target_run_id=None)],\n",
              "   'execution_time': 0.631296,\n",
              "   'run_id': '9d6f84ca-a5be-441a-86ac-325b8801e48e',\n",
              "   'output': 'The capital city of Poland is Warsaw.',\n",
              "   'reference': {'must_mention': ['Warsaw']}},\n",
              "  'd2932d26-cf60-407e-b6aa-469c823fe239': {'input': {'question': 'How to store key-value pairs in Python?'},\n",
              "   'feedback': [EvaluationResult(key='must_mention', score=True, value=None, comment=None, correction=None, evaluator_info={}, feedback_config=None, source_run_id=UUID('b78fc3aa-201b-438c-bd3c-ec16fe72299b'), target_run_id=None)],\n",
              "   'execution_time': 9.434741,\n",
              "   'run_id': '02d91328-199c-461d-91ca-9cac30901e3f',\n",
              "   'output': \"In Python, you can store key-value pairs using dictionaries. A dictionary is a collection of key-value pairs where each key is unique. Here is a basic example of how to create and use a dictionary in Python:\\n\\n### Creating a Dictionary\\nYou can create a dictionary by placing a comma-separated sequence of key-value pairs within curly braces `{}`. Each key is separated from its value by a colon `:`.\\n\\n```python\\n# Creating a dictionary\\nmy_dict = {\\n    'name': 'Alice',\\n    'age': 25,\\n    'city': 'New York'\\n}\\n```\\n\\n### Accessing Values\\nYou can access the value associated with a specific key by using square brackets `[]`.\\n\\n```python\\n# Accessing values\\nprint(my_dict['name'])  # Output: Alice\\nprint(my_dict['age'])   # Output: 25\\n```\\n\\n### Adding or Updating Key-Value Pairs\\nYou can add a new key-value pair or update an existing key-value pair by using the assignment operator `=`.\\n\\n```python\\n# Adding a new key-value pair\\nmy_dict['email'] = 'alice@example.com'\\n\\n# Updating an existing key-value pair\\nmy_dict['age'] = 26\\n```\\n\\n### Removing Key-Value Pairs\\nYou can remove a key-value pair using the `del` statement or the `pop` method.\\n\\n```python\\n# Using del statement\\ndel my_dict['city']\\n\\n# Using pop method\\nemail = my_dict.pop('email')\\n```\\n\\n### Iterating Over a Dictionary\\nYou can iterate over the keys, values, or key-value pairs in a dictionary using a `for` loop.\\n\\n```python\\n# Iterating over keys\\nfor key in my_dict:\\n    print(key)\\n\\n# Iterating over values\\nfor value in my_dict.values():\\n    print(value)\\n\\n# Iterating over key-value pairs\\nfor key, value in my_dict.items():\\n    print(f'{key}: {value}')\\n```\\n\\n### Checking if a Key Exists\\nYou can check if a key exists in a dictionary using the `in` keyword.\\n\\n```python\\nif 'name' in my_dict:\\n    print('Name is a key in the dictionary')\\n```\\n\\n### Dictionary Methods\\nDictionaries come with several useful methods, such as `get`, `keys`, `values`, `items`, `update`, and `clear`.\\n\\n```python\\n# Using get method\\nname = my_dict.get('name', 'Default Name')\\n\\n# Using keys method\\nkeys = my_dict.keys()\\n\\n# Using values method\\nvalues = my_dict.values()\\n\\n# Using items method\\nitems = my_dict.items()\\n\\n# Using update method\\nmy_dict.update({'country': 'USA'})\\n\\n# Using clear method\\nmy_dict.clear()\\n```\\n\\nThese are some of the basic operations you can perform with dictionaries in Python to store and manage key-value pairs.\",\n",
              "   'reference': {'must_mention': ['dictionary', 'dict']}},\n",
              "  'c76ac653-540a-4156-b4ab-c436449523ae': {'input': {'question': 'What capabilities agents add to LLM-based applications?'},\n",
              "   'feedback': [EvaluationResult(key='must_mention', score=False, value=None, comment=None, correction=None, evaluator_info={}, feedback_config=None, source_run_id=UUID('c6ff2a34-5870-4268-9738-4f4dc696d028'), target_run_id=None)],\n",
              "   'execution_time': 4.848421,\n",
              "   'run_id': '9654d99a-d61e-43f0-a71e-1655bf14b496',\n",
              "   'output': 'Agents add several capabilities to LLM-based (Large Language Model-based) applications, enhancing their functionality and making them more versatile and effective. Here are some key capabilities that agents can add:\\n\\n1. **Task Automation**:\\n   - Agents can automate repetitive and mundane tasks, freeing up human resources for more complex activities.\\n   - They can handle tasks such as scheduling, data entry, and basic customer service inquiries.\\n\\n2. **Contextual Understanding**:\\n   - Agents can maintain context over longer interactions, allowing for more coherent and relevant responses.\\n   - They can track user preferences and history to provide personalized experiences.\\n\\n3. **Multi-step Reasoning**:\\n   - Agents can break down complex tasks into smaller, manageable steps and execute them in sequence.\\n   - They can handle multi-turn conversations and follow through on user requests that require multiple actions.\\n\\n4. **Integration with External Systems**:\\n   - Agents can interact with various APIs and external systems to fetch or update information.\\n   - They can integrate with databases, CRM systems, and other enterprise software to provide real-time data and insights.\\n\\n5. **Decision Making**:\\n   - Agents can make decisions based on predefined rules or learned patterns.\\n   - They can evaluate different options and choose the best course of action based on the given criteria.\\n\\n6. **Natural Language Understanding and Generation**:\\n   - Agents can understand and generate human-like text, making interactions more natural and intuitive.\\n   - They can handle a wide range of languages and dialects, broadening their usability.\\n\\n7. **Learning and Adaptation**:\\n   - Agents can learn from interactions and improve over time.\\n   - They can adapt to new information and changing environments, becoming more effective with continued use.\\n\\n8. **Error Handling and Recovery**:\\n   - Agents can detect errors or misunderstandings in interactions and take corrective actions.\\n   - They can ask clarifying questions or provide alternative solutions to ensure user satisfaction.\\n\\n9. **Scalability**:\\n   - Agents can handle multiple interactions simultaneously, making them scalable solutions for high-demand environments.\\n   - They can be deployed across various platforms and devices, ensuring consistent user experiences.\\n\\n10. **Enhanced User Experience**:\\n    - Agents can provide instant responses and 24/7 availability, improving user satisfaction.\\n    - They can offer interactive and engaging experiences, such as virtual assistants, chatbots, and interactive voice response systems.\\n\\nBy incorporating these capabilities, agents can significantly enhance the functionality and user experience of LLM-based applications, making them more powerful and versatile tools for a wide range of use cases.',\n",
              "   'reference': {'must_mention': ['tool', 'reasoning']}},\n",
              "  '9973ba82-227d-4621-bc21-8d6fbec462ac': {'input': {'question': 'Who is the current president of United States'},\n",
              "   'feedback': [EvaluationResult(key='must_mention', score=True, value=None, comment=None, correction=None, evaluator_info={}, feedback_config=None, source_run_id=UUID('2161ae92-1e01-4a54-b030-cd4a6cf51e89'), target_run_id=None)],\n",
              "   'execution_time': 2.761924,\n",
              "   'run_id': 'bc55dc79-d012-4169-a9ea-b5acaa6bcb8f',\n",
              "   'output': 'The current president of the United States is Joe Biden. He is the 46th president, having taken office in 2021.',\n",
              "   'reference': {'must_mention': ['Joe', 'Biden']}},\n",
              "  '5c06c084-7759-47ca-96e9-05428534ba23': {'input': {'question': 'What is the most popular framework for distributed computations?'},\n",
              "   'feedback': [EvaluationResult(key='must_mention', score=True, value=None, comment=None, correction=None, evaluator_info={}, feedback_config=None, source_run_id=UUID('6f51bdda-4f70-4f46-a972-a38528bf8377'), target_run_id=None)],\n",
              "   'execution_time': 2.820014,\n",
              "   'run_id': '6cf7e0ed-20a7-4799-b376-12626e9e7889',\n",
              "   'output': 'The most popular frameworks for distributed computations in 2023 include:\\n\\n1. **Apache Spark**: Known for its speed and ease of use in big data processing.\\n2. **Dask**: A flexible parallel computing library for analytics.\\n3. **Ray**: A framework for building and running distributed applications.\\n\\nThese frameworks are widely used for various data science and machine learning tasks, each with its own strengths and intended use-cases.',\n",
              "   'reference': {'must_mention': ['Apache', 'Spark']}}},\n",
              " 'aggregate_metrics': None}"
            ]
          },
          "execution_count": 120,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "client.run_on_dataset(\n",
        "    dataset_name=dataset_name,\n",
        "    llm_or_chain_factory=agent_chain,\n",
        "    evaluation=eval_config,\n",
        "    verbose=True,\n",
        "    project_name=f\"RAG Pipeline - Evaluation - {uuid4().hex[0:8]}\",\n",
        "    project_metadata={\"version\": \"1.0.0\"},\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jhTNe4kWrplB"
      },
      "source": [
        "## Part 2: LangGraph with Helpfulness:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w1wKRddbIY_S"
      },
      "source": [
        "### Task 3: Adding Helpfulness Check and \"Loop\" Limits\n",
        "\n",
        "Now that we've done evaluation - let's see if we can add an extra step where we review the content we've generated to confirm if it fully answers the user's query!\n",
        "\n",
        "We're going to make a few key adjustments to account for this:\n",
        "\n",
        "1. We're going to add an artificial limit on how many \"loops\" the agent can go through - this will help us to avoid the potential situation where we never exit the loop.\n",
        "2. We'll add to our existing conditional edge to obtain the behaviour we desire."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "npTYJ8ayR5B3"
      },
      "source": [
        "First, let's define our state again - we can check the length of the state object, so we don't need additional state for this."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 146,
      "metadata": {
        "id": "-LQ84YhyJG0w"
      },
      "outputs": [],
      "source": [
        "class AgentState(TypedDict):\n",
        "  messages: Annotated[list, add_messages]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sD7EV0HqSQcb"
      },
      "source": [
        "Now we can set our graph up! This process will be almost entirely the same - with the inclusion of one additional node/conditional edge!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oajBwLkFVi1N"
      },
      "source": [
        "####üèóÔ∏è Activity #5:\n",
        "\n",
        "Please write markdown for the following cells to explain what each is doing."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M6rN7feNVn9f"
      },
      "source": [
        "**Answer**\n",
        "\n",
        "Create `graph_with_helpfulness_check` as a state machine (in the form of a StateGraph) that tracks transitions between different states (nodes). Two nodes are added to the graph:\n",
        "- \"agent\" is associated with the function `call_model` (calling an LLM)\n",
        "- \"action\" is associated with the function `tool_node` (function calling)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 147,
      "metadata": {
        "id": "6r6XXA5FJbVf"
      },
      "outputs": [],
      "source": [
        "graph_with_helpfulness_check = StateGraph(AgentState)\n",
        "\n",
        "graph_with_helpfulness_check.add_node(\"agent\", call_model)\n",
        "graph_with_helpfulness_check.add_node(\"action\", tool_node)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XZ22o2mWVrfp"
      },
      "source": [
        "**Answer**\n",
        "\n",
        "The entry point for the graph is set to \"agent\", meaning that the state machine starts with the agent node."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 148,
      "metadata": {
        "id": "HNWHwWxuRiLY"
      },
      "outputs": [],
      "source": [
        "graph_with_helpfulness_check.set_entry_point(\"agent\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rsXeF6xlaXOZ"
      },
      "source": [
        "**Answer**\n",
        "\n",
        "The `tool_call_or_helpful` function defines logic to transition between states based on the context of the conversation. It accesses the last message in the `messages` list from the agent's `state`.\n",
        "\n",
        "If the last message contains a tool call (the agent has requested the use of a tool), the function returns \"action\", which triggers a transition to the \"action\" node (`tool_node`).\n",
        "\n",
        "The `initial_query` is the first message, and the `final_response` is the last message in the list.\n",
        "\n",
        "If there are more than 10 messages in the conversation, the function returns \"END\", signaling that the conversation should terminate.\n",
        "\n",
        "The prompt template is used to check the helpfulness of the agent's response. It compares the `initial_query` and `final_response` and asks whether the final response is helpful (indicated by 'Y') or not (indicated by 'N').\n",
        "\n",
        "If the helpfulness check returns a 'Y', the conversation is deemed helpful and the function returns \"end\", which leads to the conclusion of the conversation.\n",
        "\n",
        "If the helpfulness check returns 'N', the conversation is marked as unhelpful, and the function returns \"continue\", indicating that the conversation should proceed."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 149,
      "metadata": {
        "id": "z_Sq3A9SaV1O"
      },
      "outputs": [],
      "source": [
        "from langchain_core.prompts import PromptTemplate\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "\n",
        "def tool_call_or_helpful(state):\n",
        "  last_message = state[\"messages\"][-1]\n",
        "\n",
        "  pretty_print(f\"Messages count = {len(state['messages'])}\")\n",
        "\n",
        "  if last_message.tool_calls:\n",
        "    pretty_print(\"Call to action!\")\n",
        "    return \"action\"\n",
        "\n",
        "  initial_query = state[\"messages\"][0]\n",
        "  final_response = state[\"messages\"][-1]\n",
        "\n",
        "  if len(state[\"messages\"]) > 10:\n",
        "    pretty_print(\"Limit of messages exceeded, END. The response is:\")\n",
        "    pretty_print(final_response.content)\n",
        "    return \"END\"\n",
        "\n",
        "  pretty_print(\"Evaluate the final response.\")\n",
        "\n",
        "  prompt_template = \"\"\"\\\n",
        "  Given an initial query and a final response, determine if the final response is extremely helpful or not. Please indicate helpfulness with a 'Y' and unhelpfulness as an 'N'.\n",
        "\n",
        "  Initial Query:\n",
        "  {initial_query}\n",
        "\n",
        "  Final Response:\n",
        "  {final_response}\"\"\"\n",
        "\n",
        "  prompt_template = PromptTemplate.from_template(prompt_template)\n",
        "\n",
        "  helpfulness_check_model = ChatOpenAI(model=\"gpt-4\")\n",
        "\n",
        "  helpfulness_chain = prompt_template | helpfulness_check_model | StrOutputParser()\n",
        "\n",
        "  helpfulness_response = helpfulness_chain.invoke({\"initial_query\" : initial_query.content, \"final_response\" : final_response.content})\n",
        "\n",
        "  if \"Y\" in helpfulness_response:\n",
        "    pretty_print('Helfpul! The response is:')\n",
        "    pretty_print(final_response.content)\n",
        "    return \"end\"\n",
        "  else:\n",
        "    pretty_print('Not helpful :(')\n",
        "    return \"continue\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fz1u9Vf4SHxJ"
      },
      "source": [
        "####üèóÔ∏è Activity #4:\n",
        "\n",
        "Please write what is happening in our `tool_call_or_helpful` function!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Answer**\n",
        "\n",
        "`tool_call_or_helpful` defines a conditional node in the graph. If the agent ask for a tool call, then we move to the `action` state i.e. tool calling. Note: if agent constantly calls for a tool, it will create an infinite loop.\n",
        "\n",
        "If the agent provides a response then the number of messages in the graph state is being checked: if it exceeds 10, then the response is returned to the user. If it is no more than 10 then the response becomes evaluated for its helfulness. If it is found helful, then the execution ends. If not, then execution proceeds back to the `agent` node."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 150,
      "metadata": {
        "id": "aVTKnWMbP_8T"
      },
      "outputs": [],
      "source": [
        "graph_with_helpfulness_check.add_conditional_edges(\n",
        "    \"agent\",\n",
        "    tool_call_or_helpful,\n",
        "    {\n",
        "        \"continue\" : \"agent\",\n",
        "        \"action\" : \"action\",\n",
        "        \"end\" : END\n",
        "    }\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZGDLEWOIVtK0"
      },
      "source": [
        "**Answer**\n",
        "\n",
        "Once action is completed (function calling) then the output is passed to the agent."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 151,
      "metadata": {
        "id": "cbDK2MbuREgU"
      },
      "outputs": [],
      "source": [
        "graph_with_helpfulness_check.add_edge(\"action\", \"agent\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rSI8AOaEVvT-"
      },
      "source": [
        "**Answer**\n",
        "\n",
        "Compile the graph to make it executable."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 152,
      "metadata": {
        "id": "oQldl8ERQ8lf"
      },
      "outputs": [],
      "source": [
        "agent_with_helpfulness_check = graph_with_helpfulness_check.compile()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F67FGCMRVwGz"
      },
      "source": [
        "**Answer**\n",
        "\n",
        "Call the graph with an example query."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 153,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B3oo8E-PRK1T",
        "outputId": "22470df4-9aa7-4751-95b6-d30ce71b17db"
      },
      "outputs": [
        {
          "data": {
            "text/markdown": [
              "Messages count = 2"
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/markdown": [
              "Call to action!"
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/markdown": [
              "Receiving update from node: **agent**"
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "    Content:\n"
          ]
        },
        {
          "data": {
            "text/markdown": [],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "    Additional kwargs:\n"
          ]
        },
        {
          "data": {
            "text/markdown": [
              "tool_calls"
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/markdown": [
              "refusal"
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "    Tool calls:\n"
          ]
        },
        {
          "data": {
            "text/markdown": [
              "{'name': 'duckduckgo_search', 'args': {'query': 'LoRA machine learning'}, 'id': 'call_MDG6mzJVHtsHMc42yRRXcnVq', 'type': 'tool_call'}"
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/markdown": [
              "{'name': 'duckduckgo_search', 'args': {'query': 'Tim Dettmers'}, 'id': 'call_WFSBzzNTrX3OIBYWqihD3r5V', 'type': 'tool_call'}"
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/markdown": [
              "{'name': 'duckduckgo_search', 'args': {'query': 'Attention in machine learning'}, 'id': 'call_PiZSFPUUvlE7ZTPkWbdGVZ2a', 'type': 'tool_call'}"
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\n"
          ]
        },
        {
          "data": {
            "text/markdown": [
              "Receiving update from node: **action**"
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "    Tools Used:\n"
          ]
        },
        {
          "data": {
            "text/markdown": [
              "duckduckgo_search"
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/markdown": [
              "duckduckgo_search"
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/markdown": [
              "duckduckgo_search"
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "    Content:\n"
          ]
        },
        {
          "data": {
            "text/markdown": [
              "Let's jump on LoRA. Low-Rank Adaptation of LLMs (LoRA) So, in usual fine-tuning, we. Take a pretrained model. Do Transfer Learning over new training data to slightly adjust these pre-trained weights LoRA's approach to decomposing ( Œî W ) into a product of lower rank matrices effectively balances the need to adapt large pre-trained models to new tasks while maintaining computational efficiency. The intrinsic rank concept is key to this balance, ensuring that the essence of the model's learning capability is preserved with significantly ... Low-Rank Adaptation (LoRA) is a widely-used parameter-efficient finetuning method for large language models. LoRA saves memory by training only low rank perturbations to selected weight matrices. In this work, we compare the performance of LoRA and full finetuning on two target domains, programming and mathematics. We consider both the instruction finetuning ($\\\\approx$100K prompt-response ... For example, on the subset of the GLUE dataset with T5-Base, LoRA-GA outperforms LoRA by 5.69% on average. On larger models such as Llama 2-7B, LoRA-GA shows performance improvements of 0.34, 11.52%, and 5.05% on MT-bench, GSM8K, and Human-eval, respectively. Additionally, we observe up to 2-4 times convergence speed improvement compared to ... This is where Low-Rank Adaptation (LoRA) technique appeared to be a game-changer enabling fine-tuning very big LLMs to specific task on limited resources and datasets. LoRA introduces a seemingly-simple yet powerful and cost-effective way to fine-tune LLMs and adapt them to a specific task by integrating low-rank matrices into the model's ..."
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "    Additional kwargs:\n",
            "    Tool calls:\n",
            "    No tool calls requested\n",
            "    Content:\n"
          ]
        },
        {
          "data": {
            "text/markdown": [
              "‚Äî Tim Dettmers is joining Ai2 as an AI researcher. Dettmers specializes in efficient deep learning at the intersection of machine learning, NLP, and computer systems with a focus on quantization ... Allen School Ph.D. student Tim Dettmers accepted the grand prize for QLoRA, a novel approach to finetuning pretrained models that significantly reduces the amount of GPU memory required ‚Äî from over 780GB to less than 48GB ‚Äî to finetune a 65B parameter model. With QLoRA, the largest publicly available models can be finetuned on a single ... Its purpose is to make cutting-edge research by Tim Dettmers, a leading academic expert on quantization and the use of deep learning hardware accelerators, accessible to the general public. QLoRA: One of the core contributions of bitsandbytes towards the democratization of AI. Tim Dettmers. Video. Tech Moves: AI researcher Yejin Choi leaves Univ. of Washington and Allen Institute for AI. by Todd Bishop & Taylor Soper on August 2, 2024 August 2, 2024 at 11:59 am. Tim Dettmers' research focuses on making foundation models, such as ChatGPT, accessible to researchers and practitioners by reducing their resource requirements. This involves developing novel compression and networking algorithms and building systems that allow for memory-efficient, fast, and cheap deep learning. ... Dettmers is a PhD ..."
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "    Additional kwargs:\n",
            "    Tool calls:\n",
            "    No tool calls requested\n",
            "    Content:\n"
          ]
        },
        {
          "data": {
            "text/markdown": [
              "Learn how attention mechanisms in deep learning enable models to focus on relevant information and improve performance in tasks such as machine translation, image captioning, and speech recognition. Understand the steps and components of attention mechanism architecture and see examples of its applications. The Transformer, since then, has become a popular architecture choice for a variety of tasks. It is capable of capturing long-range dependencies in data making it a powerful tool not only for NLP but also for computer vision, audio, and protein folding. Transformer: the all-powerful in the land of machine learning. Attention mechanism is a fundamental invention in artificial intelligence and machine learning, redefining the capabilities of deep learning models. This mechanism, inspired by the human mental process of selective focus, has emerged as a pillar in a variety of applications, accelerating developments in natural language processing, computer vision, and beyond. In the ever-evolving field of deep learning, one concept that has garnered significant attention (pun intended) is the Attention Mechanism. This ingenious concept has revolutionized the way neural‚Ä¶ The introduction of the Transformer model was a significant leap forward for the concept of attention in deep learning. Vaswani et al. described this model in the seminal paper titled \"Attention is All You Need\" in 2017. ... Attention mechanisms represent advancements in machine learning and computer vision, enabling models to prioritize ..."
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "    Additional kwargs:\n",
            "    Tool calls:\n",
            "    No tool calls requested\n",
            "\n",
            "\n",
            "\n"
          ]
        },
        {
          "data": {
            "text/markdown": [
              "Messages count = 6"
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/markdown": [
              "Evaluate the final response."
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/markdown": [
              "Helfpul! The response is:"
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/markdown": [
              "### LoRA (Low-Rank Adaptation)\n",
              "\n",
              "LoRA, or Low-Rank Adaptation, is a technique used in machine learning to fine-tune large language models (LLMs) efficiently. Instead of adjusting all the parameters of a pre-trained model, LoRA focuses on training only low-rank perturbations to selected weight matrices. This approach significantly reduces the memory and computational resources required for fine-tuning, making it possible to adapt large models to specific tasks even with limited resources and datasets. The method involves decomposing the weight updates into a product of lower-rank matrices, which balances the need for adaptation while maintaining computational efficiency.\n",
              "\n",
              "### Tim Dettmers\n",
              "\n",
              "Tim Dettmers is a researcher specializing in efficient deep learning, particularly at the intersection of machine learning, natural language processing (NLP), and computer systems. He is known for his work on quantization and the use of deep learning hardware accelerators. One of his notable contributions is QLoRA, a method that significantly reduces the GPU memory required to fine-tune large pre-trained models. Dettmers' research aims to make advanced AI models more accessible by developing novel compression and networking algorithms that allow for memory-efficient, fast, and cost-effective deep learning.\n",
              "\n",
              "### Attention in Machine Learning\n",
              "\n",
              "Attention mechanisms are a fundamental concept in deep learning that enable models to focus on relevant parts of the input data, improving performance in various tasks such as machine translation, image captioning, and speech recognition. The attention mechanism was popularized by the Transformer model, introduced in the seminal paper \"Attention is All You Need\" by Vaswani et al. in 2017. This mechanism allows models to capture long-range dependencies in data, making it a powerful tool not only for NLP but also for computer vision, audio processing, and other domains. Attention mechanisms have revolutionized the capabilities of deep learning models by allowing them to prioritize important information, much like the human mental process of selective focus."
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/markdown": [
              "Receiving update from node: **agent**"
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "    Content:\n"
          ]
        },
        {
          "data": {
            "text/markdown": [
              "### LoRA (Low-Rank Adaptation)\n",
              "\n",
              "LoRA, or Low-Rank Adaptation, is a technique used in machine learning to fine-tune large language models (LLMs) efficiently. Instead of adjusting all the parameters of a pre-trained model, LoRA focuses on training only low-rank perturbations to selected weight matrices. This approach significantly reduces the memory and computational resources required for fine-tuning, making it possible to adapt large models to specific tasks even with limited resources and datasets. The method involves decomposing the weight updates into a product of lower-rank matrices, which balances the need for adaptation while maintaining computational efficiency.\n",
              "\n",
              "### Tim Dettmers\n",
              "\n",
              "Tim Dettmers is a researcher specializing in efficient deep learning, particularly at the intersection of machine learning, natural language processing (NLP), and computer systems. He is known for his work on quantization and the use of deep learning hardware accelerators. One of his notable contributions is QLoRA, a method that significantly reduces the GPU memory required to fine-tune large pre-trained models. Dettmers' research aims to make advanced AI models more accessible by developing novel compression and networking algorithms that allow for memory-efficient, fast, and cost-effective deep learning.\n",
              "\n",
              "### Attention in Machine Learning\n",
              "\n",
              "Attention mechanisms are a fundamental concept in deep learning that enable models to focus on relevant parts of the input data, improving performance in various tasks such as machine translation, image captioning, and speech recognition. The attention mechanism was popularized by the Transformer model, introduced in the seminal paper \"Attention is All You Need\" by Vaswani et al. in 2017. This mechanism allows models to capture long-range dependencies in data, making it a powerful tool not only for NLP but also for computer vision, audio processing, and other domains. Attention mechanisms have revolutionized the capabilities of deep learning models by allowing them to prioritize important information, much like the human mental process of selective focus."
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "    Additional kwargs:\n"
          ]
        },
        {
          "data": {
            "text/markdown": [
              "refusal"
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "    Tool calls:\n",
            "\n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "inputs = {\"messages\" : [HumanMessage(content=\"Related to machine learning, what is LoRA? Also, who is Tim Dettmers? Also, what is Attention?\")]}\n",
        "\n",
        "async for chunk in agent_with_helpfulness_check.astream(inputs, stream_mode=\"updates\"):\n",
        "    for node, values in chunk.items():\n",
        "        # print(f\"Receiving update from node: '{node}'\")\n",
        "        # print(values[\"messages\"])\n",
        "        # print(\"\\n\\n\")\n",
        "        pretty_print(f\"Receiving update from node: **{node}**\")\n",
        "        if node == \"action\":\n",
        "          print(\"    Tools Used:\")\n",
        "          for x in values['messages']: pretty_print(f\"{x.name}\")\n",
        "        # print(\">>>\")\n",
        "        # pretty_print(str(values[\"messages\"]))\n",
        "        # print(\"<<<\")\n",
        "        for x in values[\"messages\"]:\n",
        "          print(\"    Content:\")\n",
        "          if isinstance(x.content, str):\n",
        "             pretty_print(x.content)\n",
        "          else:\n",
        "            for y in x.content: pretty_print(f\"{y}\")\n",
        "          print(\"    Additional kwargs:\")\n",
        "          for y in x.additional_kwargs: pretty_print(f\"{y}\")\n",
        "          try:\n",
        "            print(\"    Tool calls:\")\n",
        "            for y in x.tool_calls: pretty_print(f\"{y}\")\n",
        "          except:\n",
        "            print(\"    No tool calls requested\")\n",
        "        print(\"\\n\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yVmZPs6lnpsM"
      },
      "source": [
        "### Task 4: LangGraph for the \"Patterns\" of GenAI\n",
        "\n",
        "Let's ask our system about the 4 patterns of Generative AI:\n",
        "\n",
        "1. Prompt Engineering\n",
        "2. RAG\n",
        "3. Fine-tuning\n",
        "4. Agents"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 154,
      "metadata": {
        "id": "ZoLl7GlXoae-"
      },
      "outputs": [],
      "source": [
        "patterns = [\"prompt engineering\", \"RAG\", \"fine-tuning\", \"LLM-based agents\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 156,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zkh0YJuCp3Zl",
        "outputId": "0593c6f5-0a1d-41f6-960b-f32d101b3ade"
      },
      "outputs": [
        {
          "data": {
            "text/markdown": [
              "Messages count = 2"
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/markdown": [
              "Evaluate the final response."
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/markdown": [
              "Helfpul! The response is:"
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/markdown": [
              "Prompt engineering is a concept primarily associated with the field of artificial intelligence, particularly in the context of natural language processing (NLP) and large language models like GPT-3. It involves the design and crafting of prompts (input text) to elicit desired responses from AI models. The goal is to optimize the input to get the most accurate, relevant, or useful output from the model.\n",
              "\n",
              "### Key Aspects of Prompt Engineering:\n",
              "1. **Crafting Effective Prompts**: Designing prompts that are clear, specific, and structured in a way that the AI can understand and respond to appropriately.\n",
              "2. **Iterative Testing**: Continuously refining prompts based on the responses received to improve the quality and relevance of the output.\n",
              "3. **Understanding Model Behavior**: Gaining insights into how the model interprets different types of prompts and using this knowledge to guide prompt design.\n",
              "\n",
              "### Emergence of Prompt Engineering:\n",
              "Prompt engineering became more prominent with the advent of large-scale language models like OpenAI's GPT-3, which was released in June 2020. The ability of these models to generate human-like text based on prompts led to a growing interest in how to effectively communicate with them to achieve specific goals. The term \"prompt engineering\" itself started gaining traction around this time as researchers and practitioners began to explore and document best practices for interacting with these advanced AI systems.\n",
              "\n",
              "Would you like more detailed information or recent developments in prompt engineering?"
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\n"
          ]
        },
        {
          "data": {
            "text/markdown": [
              "Messages count = 2"
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/markdown": [
              "Evaluate the final response."
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/markdown": [
              "Helfpul! The response is:"
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/markdown": [
              "RAG stands for Retrieval-Augmented Generation. It is a technique in natural language processing (NLP) that combines retrieval-based methods with generative models to improve the quality and relevance of generated text. The key idea is to retrieve relevant documents or pieces of information from a large corpus and use them to inform and enhance the generation process.\n",
              "\n",
              "RAG was introduced by Facebook AI Research (FAIR) in a paper titled \"Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks,\" which was published in 2020. The technique leverages both retrieval and generation to handle knowledge-intensive tasks more effectively than either approach alone.\n",
              "\n",
              "Would you like more detailed information or specific papers on RAG?"
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\n"
          ]
        },
        {
          "data": {
            "text/markdown": [
              "Messages count = 2"
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/markdown": [
              "Evaluate the final response."
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/markdown": [
              "Helfpul! The response is:"
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/markdown": [
              "Fine-tuning is a process in machine learning where a pre-trained model is further trained on a new, often smaller, dataset to adapt it to a specific task. This approach leverages the knowledge the model has already acquired during its initial training on a large dataset, making it more efficient and effective for specialized tasks.\n",
              "\n",
              "### Key Aspects of Fine-Tuning:\n",
              "1. **Pre-trained Model**: Start with a model that has been trained on a large dataset.\n",
              "2. **New Dataset**: Use a smaller, task-specific dataset to further train the model.\n",
              "3. **Adaptation**: The model adjusts its parameters to better perform the new task while retaining the general knowledge from the initial training.\n",
              "\n",
              "### Benefits:\n",
              "- **Efficiency**: Requires less data and computational resources compared to training a model from scratch.\n",
              "- **Performance**: Often results in better performance on specific tasks due to the pre-existing knowledge in the model.\n",
              "\n",
              "### Historical Context:\n",
              "Fine-tuning became particularly prominent with the advent of deep learning and transfer learning techniques. It gained significant attention with the success of models like BERT (Bidirectional Encoder Representations from Transformers) and GPT (Generative Pre-trained Transformer), which demonstrated the effectiveness of fine-tuning for various natural language processing tasks.\n",
              "\n",
              "To pinpoint the exact timeline and key milestones, I can look up more detailed information. Would you like me to do that?"
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\n"
          ]
        },
        {
          "data": {
            "text/markdown": [
              "Messages count = 2"
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/markdown": [
              "Evaluate the final response."
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/markdown": [
              "Helfpul! The response is:"
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/markdown": [
              "LLM-based agents, or Large Language Model-based agents, are artificial intelligence systems that leverage large language models to perform a variety of tasks. These tasks can include natural language understanding, text generation, translation, summarization, question answering, and more. The core technology behind these agents is typically a deep learning model trained on vast amounts of text data to understand and generate human-like text.\n",
              "\n",
              "### Key Characteristics of LLM-based Agents:\n",
              "1. **Natural Language Processing (NLP):** They excel in understanding and generating human language.\n",
              "2. **Contextual Understanding:** They can maintain context over longer conversations or documents.\n",
              "3. **Versatility:** They can be fine-tuned for specific tasks or domains.\n",
              "4. **Scalability:** They can handle a wide range of applications from chatbots to complex decision-making systems.\n",
              "\n",
              "### Breakthrough and Evolution:\n",
              "The concept of LLM-based agents has been around for a while, but significant breakthroughs occurred with the development of models like OpenAI's GPT (Generative Pre-trained Transformer) series. Here are some key milestones:\n",
              "\n",
              "1. **2018 - GPT-1:** OpenAI released the first version of GPT, which demonstrated the potential of pre-trained language models.\n",
              "2. **2019 - GPT-2:** This version showed significant improvements in text generation and understanding, leading to widespread attention and adoption.\n",
              "3. **2020 - GPT-3:** With 175 billion parameters, GPT-3 became one of the largest and most powerful language models, capable of performing a wide range of tasks with minimal fine-tuning.\n",
              "4. **2021 and Beyond:** Continued advancements in model architecture, training techniques, and the release of other large models like Google's BERT, T5, and more have further solidified the role of LLM-based agents in AI applications.\n",
              "\n",
              "### Current Trends:\n",
              "- **Integration in Products:** LLM-based agents are now integrated into various products and services, including virtual assistants, customer service bots, and content creation tools.\n",
              "- **Research and Development:** Ongoing research aims to make these models more efficient, ethical, and capable of understanding more complex tasks.\n",
              "- **Ethical Considerations:** There is a growing focus on addressing biases, ensuring data privacy, and making these models more transparent and accountable.\n",
              "\n",
              "Would you like to know more about a specific aspect of LLM-based agents or their applications?"
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "for pattern in patterns:\n",
        "  what_is_string = f\"What is {pattern} and when did it break onto the scene??\"\n",
        "  inputs = {\"messages\" : [HumanMessage(content=what_is_string)]}\n",
        "  messages = agent_with_helpfulness_check.invoke(inputs)\n",
        "  # print(messages[\"messages\"][-1].content)\n",
        "  print(\"\\n\\n\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
